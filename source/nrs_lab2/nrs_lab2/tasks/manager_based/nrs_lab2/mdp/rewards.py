# SPDX-License-Identifier: BSD-3-Clause
from __future__ import annotations
import torch
import h5py
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv

_hdf5_trajectory = None


def load_hdf5_trajectory(env: ManagerBasedRLEnv, env_ids, file_path: str, dataset_key: str = "joint_positions"):
    """HDF5 trajectory ë°ì´í„°ë¥¼ ë¡œë“œ (reset ì‹œ 1íšŒ í˜¸ì¶œ)"""
    global _hdf5_trajectory
    with h5py.File(file_path, "r") as f:
        if dataset_key not in f:
            raise KeyError(f"[ERROR] HDF5: {dataset_key} not found. Available keys: {list(f.keys())}")
        data = f[dataset_key][:]  # [T, D]
    _hdf5_trajectory = torch.tensor(data, dtype=torch.float32, device=env.device)


def get_hdf5_target(env: ManagerBasedRLEnv) -> torch.Tensor:
    global _hdf5_trajectory
    if _hdf5_trajectory is None:
        raise RuntimeError("HDF5 trajectory not loaded. Did you register load_hdf5_trajectory?")

    T = _hdf5_trajectory.shape[0]      # HDF5 ê¸¸ì´
    E = env.max_episode_length         # episode step ìˆ˜

    # episode ë‚´ë¶€ step counter ì‚¬ìš© (reset ì‹œ 0ìœ¼ë¡œ ëŒì•„ê°)
    step = env.episode_length_buf[0].item()

    # ðŸ”‘ HDF5 ì¸ë±ìŠ¤ë¥¼ episode ì§„í–‰ë„ì— ë§žì¶° ìŠ¤ì¼€ì¼ë§
    idx = min(int(step / E * T), T - 1)

    return _hdf5_trajectory[idx]


# -------------------
# Reward functions
# -------------------

def joint_target_error_strict(env: ManagerBasedRLEnv, scale: float = 50.0) -> torch.Tensor:
    """
    ê° jointë³„ targetê³¼ í˜„ìž¬ê°’ ì°¨ì´ê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í° ë³´ìƒ
    - ì¡°ì¸íŠ¸ë³„ë¡œ exp(-scale * (diff^2)) ê³„ì‚° í›„ í‰ê·  ì‚¬ìš©
    - ë””ë²„ê¹… ì¶œë ¥ì€ ê¸°ì¡´ í˜•ì‹ ìœ ì§€
    """
    q = env.scene["robot"].data.joint_pos  # [num_envs, num_joints]
    target = get_hdf5_target(env).unsqueeze(0).repeat(env.num_envs, 1)  # [num_envs, num_joints]

    # per-joint reward (exp shaping)
    diffs = q - target
    mse = torch.mean(diffs ** 2, dim=-1)  # ê¸°ì¡´ MSEë„ ê³„ì‚° (ì¶œë ¥ìš©)
    rewards = torch.exp(-scale * (diffs ** 2))  # [num_envs, num_joints]

    # ì „ì²´ reward = í‰ê· 
    reward = torch.mean(rewards, dim=-1)

    # âœ… ë””ë²„ê¹… ì¶œë ¥ (ì›ëž˜ í˜•ì‹ ìœ ì§€)
    if env.common_step_counter % 100 == 0:
        current_time = env.common_step_counter * env.step_dt
        print(f"[Step {env.common_step_counter} | Time {current_time:.2f}s] "
              f"Target[0]: {target[0].cpu().numpy()} "
              f"Current[0]: {q[0].cpu().numpy()} "
              f"MSE[0]: {mse[0].item():.6f}, Reward[0]: {reward[0].item():.6f}")

    return reward


# -------------------
# Termination function
# -------------------

def reached_end(env: ManagerBasedRLEnv) -> torch.Tensor:
    """HDF5 trajectory ëì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ"""
    global _hdf5_trajectory
    if _hdf5_trajectory is None:
        return torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)

    T = _hdf5_trajectory.shape[0]
    return torch.tensor(env.common_step_counter >= (T - 1),
                        dtype=torch.bool, device=env.device).repeat(env.num_envs)
